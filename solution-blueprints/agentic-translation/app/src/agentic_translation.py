# Copyright © Advanced Micro Devices, Inc., or its affiliates.
#
# SPDX-License-Identifier: MIT

import copy
from typing import Any, Iterator

import tiktoken
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage
from langchain_core.prompts.chat import (
    BaseMessagePromptTemplate,
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from prompts import (
    CRITIQUE_SYSTEM_MESSAGE,
    JUDGE_SYSTEM_MESSAGE,
    JUDGEMENT_PROMPT,
    PROMPT_HISTORY,
    REFLECTION_PROMPT,
    TRANSLATION_INSTRUCTION,
    TRANSLATION_PROMPT,
    TRANSLATOR_SYSTEM_MESSAGE,
)
from transformers import AutoTokenizer


def get_llm_response(llm: BaseChatModel, system_prompt: str, human_prompts: list[str], context: dict) -> BaseMessage:
    """
    Generates a response from a language model (LLM) using a system prompt, a list of human prompts, and a context.
    Args:
        llm (BaseChatModel): The language model to generate the response from.
        system_prompt (str): The system-level prompt to guide the LLM's behavior.
        human_prompts (list[str]): A list of prompts representing human input or conversation turns.
        context (dict): A dictionary containing context variables to be used in the prompt templates.
    Returns:
        BaseMessage: The response message generated by the LLM.
    """
    # Create prompt from messages and context
    messages: list[BaseMessagePromptTemplate] = [SystemMessagePromptTemplate.from_template(system_prompt)]
    for human_prompt in human_prompts:
        messages.append(HumanMessagePromptTemplate.from_template(human_prompt))
    prompt_template = ChatPromptTemplate.from_messages(messages)
    prompt = prompt_template.invoke(context)
    # print(prompt)  # for debugging

    # Query LLM and return response
    response = llm.invoke(prompt)
    return response


def action_agent(context: dict, llm: BaseChatModel) -> BaseMessage:
    """
    Translation agent which queries a language model based on the provided context and returns the model's response.
    Args:
        context (dict): A dictionary containing translation parameters, which may include:
            - 'source_language' (str): The language to translate from.
            - 'target_language' (str): The language to translate to.
            - 'source_text' (str): The text to be translated.
            - 'instruction' (str, optional): Additional instructions for the translation.
            - 'translation' (str, optional): A previous translation to consider.
            - 'critique' (str, optional): Critique of the previous translation.
        llm (BaseChatModel): The language model to use for generating the translation.
    Returns:
        BaseMessage: The response from the language model containing the translation.
    """
    translator_system_message = context.get("translator_system_message", TRANSLATOR_SYSTEM_MESSAGE)
    translation_prompt = context.get("translation_prompt", TRANSLATION_PROMPT)
    prompts = [translation_prompt]
    if "instruction" in context and context["instruction"]:
        translation_instruction = context.get("translation_instruction", TRANSLATION_INSTRUCTION)
        prompts.append(translation_instruction)
    # Add all previous translations and critiques as history
    if "history" in context and context["history"]:
        prompt_critique_history = context.get("prompt_history", PROMPT_HISTORY)
        prompts.append(prompt_critique_history)
    return get_llm_response(llm, translator_system_message, prompts, context)


def critique_agent(context: dict, llm: BaseChatModel) -> BaseMessage:
    """
    Critique agent which provides improvement suggestions for a translation using a language model.
    Args:
        context (dict): A dictionary containing the following keys:
            - 'source_language' (str): The language of the source text.
            - 'target_language' (str): The language of the translation.
            - 'source_text' (str): The original text to be translated.
            - 'translation' (str): The translated text to be critiqued.
            - 'instruction' (str, optional): Additional instructions for the translation, if any.
        llm (BaseChatModel): The language model to use for generating the critique.
    Returns:
        BaseMessage: The response from the language model containing a list of specific, constructive suggestions for improving the translation.
    """
    critique_system_message = context.get("critique_system_message", CRITIQUE_SYSTEM_MESSAGE)
    reflection_prompt = context.get("reflection_prompt", REFLECTION_PROMPT)
    prompts = [reflection_prompt]
    if "instruction" in context and context["instruction"]:
        translation_instruction = context.get("translation_instruction", TRANSLATION_INSTRUCTION)
        prompts.append(translation_instruction)
    # Add all previous translations and critiques as history
    if "history" in context and context["history"]:
        prompt_critique_history = context.get("prompt_history", PROMPT_HISTORY)
        prompts.append(prompt_critique_history)
    return get_llm_response(llm, critique_system_message, prompts, context)


def judgement_agent(context: dict, llm: BaseChatModel) -> BaseMessage:
    """
    Judgment agent which evaluates the quality of a translation based on the provided source text, translation, and critique.
    Args:
        context (dict): A dictionary containing the following keys:
            - 'source_language' (str): The language of the source text.
            - 'target_language' (str): The language of the translation.
            - 'source_text' (str): The original text to be translated.
            - 'translation' (str): The translated text.
            - 'critique' (str): Feedback or critique on the translation.
            - 'instruction' (str, optional): Additional translation instructions, if any.
        llm (BaseChatModel): The language model to use for generating the judgment.
    Returns:
        BaseMessage: The response from the language model, which is either "Yes" or "No" indicating whether the translation is of high quality considering the critique.
    """
    judge_system_message = context.get("judge_system_message", JUDGE_SYSTEM_MESSAGE)
    judgement_prompt = context.get("judgement_prompt", JUDGEMENT_PROMPT)
    prompts = [judgement_prompt]
    if "instruction" in context and context["instruction"]:
        translation_instruction = context.get("translation_instruction", TRANSLATION_INSTRUCTION)
        prompts.append(translation_instruction)
    # Add all previous translations and critiques as history
    if "history" in context and context["history"]:
        prompt_critique_history = context.get("prompt_history", PROMPT_HISTORY)
        prompts.append(prompt_critique_history)
    return get_llm_response(llm, judge_system_message, prompts, context)


def trilateral_collaboration_workflow(context: dict, llm: BaseChatModel, max_iterations=3) -> Iterator[tuple[str, Any]]:
    """
    Executes a trilateral collaboration workflow involving action, critique, and judgment agents.
    Args:
        context (dict): The shared context dictionary passed between agents, containing relevant information and accumulating responses.
        llm (BaseChatModel): The language model instance used by the agents to generate responses.
        max_iterations (int, optional): The maximum number of workflow iterations. Defaults to 3.
    Yields:
        Iterator[tuple[str, Any]]: Tuples indicating the agent type ("action", "critique", or "judge") and the corresponding agent's response content.
    """
    m = 0
    # Initialize history lists
    translations: list[str] = []
    critiques: list[str] = []
    while m < max_iterations:
        m += 1
        # Prepare history string for prompt
        history_entries = []
        for t, c in zip(translations, critiques):
            history_entries.append(f"<TRANSLATION>{t}</TRANSLATION>\n<CRITIQUE>{c}</CRITIQUE>")
        context["history"] = "\n".join(history_entries)
        # Action agent generates response
        action_response = action_agent(context, llm)
        context["translation"] = action_response.content
        translations.append(str(action_response.content))
        yield ("action", action_response.content)

        # Critique agent generates feedback
        critique_response = critique_agent(context, llm)
        context["critique"] = critique_response.content
        critiques.append(str(critique_response.content))
        yield ("critique", critique_response.content)

        if m > 1:
            # Judgment agent evaluates response quality
            judgement_response = judgement_agent(context, llm)
            yield ("judge", judgement_response.content)
            # Terminate the workflow early if the judgment is "Yes" (translation is of high quality)
            if isinstance(judgement_response.content, str) and judgement_response.content.lower().startswith("yes"):
                return


def calculate_chunk_size(token_count: int, token_limit: int) -> int:
    # This function is adapted from the open-source project Translation Agent: Agentic translation using reflection workflow
    # Original source: https://github.com/andrewyng/translation-agent
    # Original author: Andrew Ng, Joaquin Dominguez, Nedelina Teneva, John Santerre, Ralf D. Müller,
    # License: MIT https://github.com/andrewyng/translation-agent?tab=MIT-1-ov-file
    # Modifications: No modification
    """
    Calculate the chunk size based on the token count and token limit.

    Args:
        token_count (int): The total number of tokens.
        token_limit (int): The maximum number of tokens allowed per chunk.

    Returns:
        int: The calculated chunk size.

    Description:
        This function calculates the chunk size based on the given token count and token limit.
        If the token count is less than or equal to the token limit, the function returns the token count as the chunk size.
        Otherwise, it calculates the number of chunks needed to accommodate all the tokens within the token limit.
        The chunk size is determined by dividing the token limit by the number of chunks.
        If there are remaining tokens after dividing the token count by the token limit,
        the chunk size is adjusted by adding the remaining tokens divided by the number of chunks.

    Example:
        >>> calculate_chunk_size(1000, 500)
        500
        >>> calculate_chunk_size(1530, 500)
        389
        >>> calculate_chunk_size(2242, 500)
        496
    """

    if token_count <= token_limit:
        return token_count

    num_chunks = (token_count + token_limit - 1) // token_limit
    chunk_size = token_count // num_chunks

    remaining_tokens = token_count % token_limit
    if remaining_tokens > 0:
        chunk_size += remaining_tokens // num_chunks

    return chunk_size


def multichunk_trilateral_collaboration_workflow(
    context: dict, llm: BaseChatModel, max_iterations=2
) -> Iterator[tuple[str, Any]]:
    """
    Improves the translation of multiple text chunks based on the initial translation and reflection.

    Args:
        context (dict): The shared context dictionary passed between agents, containing relevant information and accumulating responses.
        llm (BaseChatModel): The language model instance used by the agents to generate responses.
        max_iterations (int, optional): The maximum number of workflow iterations. Defaults to 3.
    Yields:
        Iterator[tuple[str, Any]]: Tuples indicating the agent type ("action", "critique", or "judge") and the corresponding agent's response content.
    """
    # Iterate over chunks
    for i in range(len(context["source_text_chunks"])):

        # Create a context dict with one chunk only
        context_chunk = copy.deepcopy(context)
        context_chunk["source_text"] = str(context["source_text_chunks"][i])

        m = 0
        # Initialize history lists
        translations: list[str] = []
        critiques: list[str] = []
        while m < max_iterations:
            m += 1
            # Prepare history string for prompt
            history_entries = []
            for t, c in zip(translations, critiques):
                history_entries.append(f"<TRANSLATION>{t}</TRANSLATION>\n<CRITIQUE>{c}</CRITIQUE>")
            context_chunk["history"] = "\n".join(history_entries)

            # Action agent generates response
            action_response = action_agent(context_chunk, llm)
            context_chunk["translation"] = action_response.content
            translations.append(str(action_response.content))

            # Critique agent generates feedback
            critique_response = critique_agent(context_chunk, llm)
            context_chunk["critique"] = critique_response.content
            critiques.append(str(critique_response.content))

            # Judgment agent evaluates response quality
            judgement_response = judgement_agent(context_chunk, llm)
            context_chunk["judge"] = judgement_response.content
            # Terminate the workflow early if the judgment is "Yes" (translation is of high quality)
            if isinstance(judgement_response.content, str) and judgement_response.content.lower().startswith("yes"):
                break

        # Yield the results from the last iteration of each chunk
        yield ("action", str(context_chunk["translation"]))
        yield ("critique", str(context_chunk["critique"]))
        yield ("judge", str(context_chunk["judge"]))

    return


def preprocess_multichunk_text(context: dict, num_tokens_in_text: int, max_tokens: int, model_name: str = ""):
    """
    - Calculates number of tokens per chunk
    - splits the text with number of tokens in each chunk
    - add list of chunks to context dict
    - Modifies the context dict with new chunked source text.

    Args:
        context (dict): Dictionary including text to be translate, language etc.
        num_tokens_in_text (int): number of tokens in the text
        max_tokens (int): The maximum number of tokens allowed per chunk.
        model_name (str): name of model to use for tokenization

    """
    # Calculate the number of tokens per chunk based on max_tokens
    token_size = calculate_chunk_size(token_count=num_tokens_in_text, token_limit=max_tokens)

    # Initialise text splitter, will split on "good places" in the text using llm
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
            tokenizer=tokenizer,
            chunk_size=token_size,
            chunk_overlap=0,
        )
    except Exception as autotokenizer_exception:
        try:
            text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
                model_name=model_name,
                chunk_size=token_size,
                chunk_overlap=0,
            )

        except Exception as tiktoken_exception:
            text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
                model_name="gpt-4",
                chunk_size=token_size,
                chunk_overlap=0,
            )

    # Splits the text into chunks
    source_text_chunks = text_splitter.split_text(context["source_text"])

    # Save list of chunks in context dict
    context["source_text_chunks"] = source_text_chunks

    return


def select_translate_method(
    context: dict, llm: BaseChatModel, max_iterations: int = 2, max_tokens: int = 2000
) -> Iterator[tuple[str, Any]]:
    """
    Calculates the number of tokens in the text,
    will make a choice if the text need to be split into chunks,
    then call different translation functions depending on the size.

    Args:
        context (dict): Dictionary including text to be translate, language etc.
        llm (BaseChatModel): The language model instance used by the agents to generate responses.
        max_iterations (int, optional): The maximum number of workflow iterations.
        max_tokens (int): The maximum number of tokens allowed per chunk.
    Yields:
        Iterator[tuple[str, Any]]: Tuples indicating the agent type ("action", "critique", or "judge") and the corresponding agent's response content.
    """

    model_name = getattr(llm, "model_name", "")
    # Calculate number of tokens in text
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        num_tokens_in_text = len(tokenizer.tokenize(str(context["source_text"])))
    except Exception as autotokenizer_exception:
        print(f"AutoTokenizer.from_pretrained failed: {autotokenizer_exception}")
        print(f"Falling back to tiktoken with model_name: {model_name}")
        try:
            encoding = tiktoken.encoding_for_model(model_name)
        except Exception as tiktoken_exception:
            print(f"tiktoken.encoding_for_model failed: {tiktoken_exception}")
            print("Falling back to cl100k_base")
            encoding = tiktoken.get_encoding("cl100k_base")
        num_tokens_in_text = len(encoding.encode(str(context["source_text"])))

    if num_tokens_in_text < max_tokens:
        # Translating text as a single chunk
        final_translation = " "
        for agent, content in trilateral_collaboration_workflow(context, llm, max_iterations):
            yield (agent, content)

            if agent == "action":
                # Only save the last action content
                final_translation = str(content)
        yield ("final translation", final_translation)

    else:
        # Translating text as multiple chunks
        preprocess_multichunk_text(context, num_tokens_in_text, max_tokens, model_name)

        yield (
            "System",
            "The source text is long and will be translated by chunks. Only the last outputs of each agent is returned.",
        )

        final_translation = " "
        for agent, content in multichunk_trilateral_collaboration_workflow(context, llm, max_iterations):
            yield (agent, content)
            if agent == "action":
                final_translation = final_translation + str(content)

        yield ("Final Translation", final_translation)
    return
